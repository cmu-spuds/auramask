{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 20:22:58.538791: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-11 20:22:58.577209: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-11 20:22:58.577233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-11 20:22:58.578325: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-11 20:22:58.584942: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 20:22:59.459338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/unet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from auramask.losses.perceptual import PerceptualLoss\n",
    "from auramask.losses.embeddistance import EmbeddingDistanceLoss\n",
    "from auramask.losses.ssim import SSIMLoss\n",
    "from auramask.losses.aesthetic import AestheticLoss\n",
    "from auramask.models.face_embeddings import FaceEmbedEnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Victim Models (F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [FaceEmbedEnum.ARCFACE, FaceEmbedEnum.DEEPID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbones are 'vgg', 'alex', 'squeeze'\n",
    "backbone = 'vgg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Loss\n",
    "$$\n",
    "loss \\leftarrow \\dfrac{1}{\\left\\|\\mathbb{F}\\right\\|} \\sum^{\\mathbb{F}}_{f} - \\dfrac{f(x) \\cdot f(x_{adv})} {\\left\\| f(x)\\right\\|_{2}\\left\\| f(x_{adv})\\right\\|_{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 20:23:01.090498: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-11 20:23:01.128921: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-11 20:23:01.130349: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-11 20:23:01.133943: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-11 20:23:01.135479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-11 20:23:01.136863: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-11 20:23:01.259741: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-11 20:23:01.261218: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-11 20:23:01.262483: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-03-11 20:23:01.262527: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-11 20:23:01.263810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15983 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:00:04.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.engine.functional.Functional object at 0x7f42083ddc10>\n",
      "<keras.src.engine.functional.Functional object at 0x7f420836f090>\n"
     ]
    }
   ],
   "source": [
    "F_loss = EmbeddingDistanceLoss(F=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "F_set = FaceEmbedEnum.build_F(F)\n",
    "cossim = CosineSimilarity(axis=-1)\n",
    "def f_cosine_similarity(x, x_adv, f):\n",
    "  emb_t = f(x)\n",
    "  emb_adv = f(x_adv)\n",
    "  dist = cossim(emb_t, emb_adv)\n",
    "  dist = tf.negative(dist)\n",
    "  return dist\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "N = len(F)\n",
    "def F_loss(x, x_adv):\n",
    "  loss = 0.0\n",
    "  for f in F_set:\n",
    "    loss = tf.add(loss, f_cosine_similarity(x, x_adv, f))\n",
    "  loss = tf.divide(loss, N)\n",
    "  return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual Loss ($L_{pips}$)\n",
    "$$\n",
    "loss \\leftarrow loss + \\lambda L_{pips}(x_{adv}, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips = PerceptualLoss(backbone=backbone, spatial=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aesthetic Loss ($L_{nima}$)\n",
    "$$\n",
    "loss \\leftarrow loss + \\phi L_{nima}(x_{adv}, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<auramask.models.nima.NIMA object at 0x7f4204293890>\n"
     ]
    }
   ],
   "source": [
    "laes = AestheticLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSIM Loss ($L_{nima}$)\n",
    "$$\n",
    "loss \\leftarrow loss + \\phi L_{ssim}(x_{adv}, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssimloss = SSIMLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reface_loss(x, x_adv):\n",
    "  floss = F_loss(x, x_adv)\n",
    "  lpipsloss = lpips(x, x_adv)\n",
    "  return tf.add(floss, lpipsloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auramask_loss(x, x_adv):\n",
    "  floss = F_loss(x, x_adv)\n",
    "  lpipsloss = lpips(x, x_adv)\n",
    "  aesthetic = laes(x, x_adv)\n",
    "  return tf.add(floss, lpipsloss, aesthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Testing the giving parts of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from keras_cv.layers import Resizing, Rescaling, Augmenter, RandomContrast, RandomColorDegeneration, RandomColorJitter, RandAugment\n",
    "from tensorflow.data import AUTOTUNE\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.\n"
     ]
    }
   ],
   "source": [
    "ds, info = tfds.load('lfw',\n",
    "                     decoders=tfds.decode.PartialDecoding({\n",
    "                       'image': True,\n",
    "                     }),\n",
    "                     with_info=True,\n",
    "                     download=True,\n",
    "                     as_supervised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = int.from_bytes(\"Pochita Lives\".encode())\n",
    "\n",
    "augmenter = Augmenter(\n",
    "  [\n",
    "    Rescaling(1./255),\n",
    "    Resizing(256,256),\n",
    "  ]\n",
    ")\n",
    "\n",
    "noise_addition = Augmenter(\n",
    "  [\n",
    "    RandAugment(\n",
    "      value_range=(0,1),\n",
    "      augmentations_per_image=3,\n",
    "      magnitude=0.5,\n",
    "      seed=SEED\n",
    "      )\n",
    "  ]\n",
    ")\n",
    "\n",
    "def preprocess_data(images, augment=True):\n",
    "  outputs_1 = augmenter(images)\n",
    "  if augment:\n",
    "    outputs_2 = noise_addition(outputs_1)\n",
    "  else:\n",
    "    outputs_2 = outputs_1\n",
    "  return outputs_1, outputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(BATCH_SIZE).map(\n",
    "  lambda x: preprocess_data(x['image'], True),\n",
    "  num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Loss Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 20:23:21.765559: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-03-11 20:23:22.501980: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-03-11 20:23:22.563011: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-11 20:23:22.813693: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.3457596, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(1):\n",
    "  print(auramask_loss(batch[0], batch[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses:\n",
      " \tF-Loss: 0.869\n",
      " \tLpips: 0.533\n",
      " \tAesthetic: 0.647\n",
      " \tSSIM: 0.868\n",
      "\n",
      "Losses:\n",
      " \tF-Loss: 0.904\n",
      " \tLpips: 0.439\n",
      " \tAesthetic: 0.661\n",
      " \tSSIM: 0.655\n",
      "\n",
      "Losses:\n",
      " \tF-Loss: 0.851\n",
      " \tLpips: 0.490\n",
      " \tAesthetic: 0.643\n",
      " \tSSIM: 0.752\n",
      "\n",
      "Losses:\n",
      " \tF-Loss: 0.888\n",
      " \tLpips: 0.447\n",
      " \tAesthetic: 0.646\n",
      " \tSSIM: 0.780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 20:23:29.376005: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses:\n",
      " \tF-Loss: 0.877\n",
      " \tLpips: 0.470\n",
      " \tAesthetic: 0.633\n",
      " \tSSIM: 0.752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(5):\n",
    "  floss = F_loss(batch[0], batch[1])\n",
    "  ploss = lpips(batch[0], batch[1])\n",
    "  aloss = laes(batch[0], batch[1])\n",
    "  ssiml = ssimloss(batch[0], batch[1])\n",
    "  print(\"Losses:\\n\", \"\\tF-Loss: %0.3f\\n\"%(floss), \"\\tLpips: %0.3f\\n\"%(ploss), \"\\tAesthetic: %0.3f\\n\"%(aloss), \"\\tSSIM: %0.3f\\n\"%(ssiml))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
