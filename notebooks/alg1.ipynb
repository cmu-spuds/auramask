{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 19:53:59.476334: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-23 19:53:59.513676: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-23 19:53:59.513700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-23 19:53:59.514629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-23 19:53:59.520922: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-23 19:54:00.428161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/miniconda/envs/unet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "from tensorflow.data import AUTOTUNE\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import auramask as am\n",
    "\n",
    "from git import Repo\n",
    "\n",
    "from keras_cv.layers import Resizing, Rescaling, Augmenter\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, Callback\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "branch = Repo(\"./\").active_branch.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Victim Models (F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [am.models.FaceEmbedEnum.ARCFACE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Dataset (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`FeatureConnector.dtype` is deprecated. Please change your code to use NumPy with the field `FeatureConnector.np_dtype` or use TensorFlow with the field `FeatureConnector.tf_dtype`.\n",
      "2024-02-23 19:54:02.652679: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 19:54:02.687731: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 19:54:02.689182: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 19:54:02.692825: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 19:54:02.694260: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 19:54:02.695634: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 19:54:02.819432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 19:54:02.820940: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 19:54:02.822310: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-02-23 19:54:02.822363: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 19:54:02.823741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17735 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:00:04.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "ds, info = tfds.load(\n",
    "    \"lfw\",\n",
    "    decoders=tfds.decode.PartialDecoding(\n",
    "        {\n",
    "            \"image\": True,\n",
    "        }\n",
    "    ),\n",
    "    with_info=True,\n",
    "    download=True,\n",
    "    as_supervised=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = Augmenter(\n",
    "    [\n",
    "        Rescaling(1.0 / 255),\n",
    "        Resizing(256, 256),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_data(images, augment=True):\n",
    "    inputs = {\"images\": images}\n",
    "    outputs = augmenter(inputs)\n",
    "    return outputs[\"images\"], outputs[\"images\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2e-4\n",
    "epsilon = 0.03\n",
    "l_weight = 0.0\n",
    "BATCH_SIZE = 32\n",
    "# EPOCH = 500  # ReFace training\n",
    "EPOCH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    train_ds.batch(BATCH_SIZE)\n",
    "    .map(lambda x: preprocess_data(x[\"image\"]), num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized ATN ($N_{theta}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_loss = am.losses.ReFaceLoss(F=F, l=l_weight)\n",
    "model = am.models.AuraMask(n_filters=32, n_dims=3, eps=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lpips = am.metrics.PerceptualSimilarity(model=t_loss.lpips.model)\n",
    "# floss = am.metrics.EmbeddingDistance(F=t_loss.embeddist.F, F_set=t_loss.embeddist.F_set)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=alpha),\n",
    "    loss=[t_loss.lpips, t_loss.embeddist],\n",
    "    loss_weights=[l_weight, 1.0],\n",
    "    run_eagerly=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 19:54:08.540215: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "SEED = None\n",
    "for x, _ in train_ds.take(1):\n",
    "    sample = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    hp.HParam(\"F\"): str([f.name.lower() for f in F]),\n",
    "    hp.HParam(\"optimizer\"): \"adam\",\n",
    "    hp.HParam(\"alpha\"): alpha,\n",
    "    hp.HParam(\"epsilon\"): epsilon,\n",
    "    hp.HParam(\"lambda\"): l_weight,\n",
    "    hp.HParam(\"Batch Size\"): BATCH_SIZE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        super().__init__()\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        hp.hparams(hparams)\n",
    "        tf.summary.image(\"Original\", sample, max_outputs=10, step=0)\n",
    "        return super().on_train_begin(logs)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch % 20 == 0:\n",
    "            tf.summary.image(\n",
    "                \"Augmented/%d\" % self.epoch,\n",
    "                self.model(sample),\n",
    "                max_outputs=1,\n",
    "                step=batch,\n",
    "            )\n",
    "        return super().on_train_batch_begin(batch, logs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        tf.summary.image(\n",
    "            \"Augmented/epoch\", self.model(sample), max_outputs=10, step=epoch\n",
    "        )\n",
    "        self.epoch += 1\n",
    "        return super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"logs/nocrop/%s/%s/%s\" % (\n",
    "    branch,\n",
    "    datetime.now().strftime(\"%Y%m%d\"),\n",
    "    datetime.now().strftime(\"%H%M%S\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=LOG_DIR, write_images=True, update_freq=1, histogram_freq=1\n",
    ")\n",
    "early_stop = EarlyStopping(monitor=\"loss\", patience=3)\n",
    "img_call = ImageCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 19:54:08.758639: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-23 19:54:09.629918: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-02-23 19:54:09.698471: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-23 19:54:38.910767: I external/local_xla/xla/service/service.cc:168] XLA service 0x356a9480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-23 19:54:38.910796: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A10, Compute Capability 8.6\n",
      "2024-02-23 19:54:38.915507: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708718079.013081  240343 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "training_history = model.fit(\n",
    "    x=train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[tensorboard_callback, early_stop, img_call],\n",
    "    epochs=EPOCH,\n",
    "    verbose=0,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
